{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9591fd4a-ff45-4016-9235-f05fa956e86e",
   "metadata": {},
   "source": [
    "**Learning outcomes**\n",
    "\n",
    "- Enumerate the components of an ML pipeline\n",
    "    - Reusable data exploration scripts\n",
    "    - Reusable data prep\n",
    "    - Reusable model training\n",
    "\n",
    "- Build each component using standard python libraries:\n",
    "    - Build reusable data exploration scripts\n",
    "    - Build reusable data prep scripts\n",
    "    - Build reusable model training scripts\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Data scientists are the key stakeholders when in comes to creating machine learning or statistical models. The usual process followed by most data science teams is to pull raw data into many jupyter notebooks and then do the usual steps of cleaiing and preparing the data.\n",
    "\n",
    "![](../images/ops1.png)\n",
    "\n",
    "But how are these models that a datascience team creates used as part of a bigger product? The model building journey doesn't end the moment a model is trained and sufficient model performance is achieved. One needs to structure the data exploration, data perparation and model training tasks as separate functioning modules.\n",
    "\n",
    "![](../images/ops2.png)\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "One of the first things that one needs to do is to create modelling pipeline. A pipeline consists of the following components:\n",
    "\n",
    "1. Reusable scripts to explore data\n",
    "2. Reusable scripts to prepare data\n",
    "3. Reusable scripts to train a model\n",
    "\n",
    "\n",
    "To demonstrate how a training pipeline works we will use a notebook that already contains the code for data exploration, preparation and model training.\n",
    "\n",
    "\n",
    "<a href=\"Data Scientist Notebook.ipynb\">Notebook</a>\n",
    "\n",
    "Use the notebook linked above to create a training pipeline.\n",
    "\n",
    "**1.Creating a data exploration script**\n",
    "\n",
    "We will modify the code in the original notebook, more specifically the part where we do data exploration.\n",
    "\n",
    "![](../images/ops3.png)\n",
    "\n",
    "We will try to create a python script to do data exploration for us. Below we describe some parts that will go into making this\n",
    "script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d4e5e-e4b8-4862-ad3d-438c4d43e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/credit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f57f1-6ddf-454c-9d86-d06b455810a5",
   "metadata": {},
   "source": [
    "The typical scenario that a data exploration script should handle, will be when new training data or inference data arrives and a preliminary data exploration needs to be done.\n",
    "\n",
    "For a similar but new dataset, we need our data exploration script to do the following:\n",
    "\n",
    "1. Validate the column names and number\n",
    "2. If the dataset is being used for model inference, validate the levels in a categorical variable\n",
    "3. Save data exploration plots given in original notebook\n",
    "4. Save a report on missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7cacf-49a2-4bbb-8708-bb93404b20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a function to validate the column names and number (Instructor guided)\n",
    "\n",
    "\n",
    "### Define a function to validate the categorical levels  (Instructor guided)\n",
    "\n",
    "\n",
    "### Decile computation (Instructor guided)\n",
    "\n",
    "\n",
    "### Function to create and save exploration plots (Instructor guided)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042dd53-b5c0-4b4b-8a86-9fcce4eb73ef",
   "metadata": {},
   "source": [
    "**Python Data Exporation Script** {todo as class excercise guided by instructor}\n",
    "\n",
    "\n",
    "**2. Creating Data Preparation Script**\n",
    "\n",
    "This script should be able to:\n",
    "\n",
    "- Remove missing values, in our specific case drop the missing values\n",
    "- Create dummy variables\n",
    "- Split data into target and predictor matrices\n",
    "- Create a train test split\n",
    "- Save the train/test data to assets/data folder\n",
    "\n",
    "We will continue to use the `Data Scientist Notebook` as a reference to build the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e68e2-314c-421c-b153-265438590286",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function to create target and predictor matrices (Instructor guided)\n",
    "\n",
    "\n",
    "## train test split(Instructor guided)\n",
    "\n",
    "## save train test data as numpy arrays(Instructor guided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe6a52-c71b-4a6d-8cfd-d4c8b82be751",
   "metadata": {},
   "source": [
    "**3. Creating model training script**\n",
    "\n",
    "Lastly we can create a model training script, once the `exploration.py` and `prep.py` have run we can run a `train.py` script.\n",
    "\n",
    "This script should be able to:\n",
    "\n",
    "- Read the data prepared and stored in `assets/data/` directory.\n",
    "- Train a decision tree model by doing grid search\n",
    "- Save the trained model and classification metrics in `assets/model` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e47f69-6a37-4fd2-9aa7-7a4af4228bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Read data saved in the data directory (Instructor guided)\n",
    "\n",
    "## Model train function (Instructor guided)\n",
    "\n",
    "## Model Save function (Instructor guided)\n",
    "\n",
    "## Calculate metrics (Instructor guided)\n",
    "\n",
    "## Save metrics (Instructor guided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffb7e8-07d7-4749-8c24-c448412bcc8b",
   "metadata": {},
   "source": [
    "One we have an ML training pipeline, we can train models fast. But another aspect of operationalizing ML is to have an inference pipeline.\n",
    "\n",
    "An inference pipeline should be able to do the following:\n",
    "\n",
    "1. Take inference data, do necesary data perp\n",
    "2. Read serialized model and run predictions\n",
    "3. Save summary stats of the data and predictions with a date-time stamp\n",
    "\n",
    "\n",
    "Below we build an inference pipeline:\n",
    "\n",
    "1. We will modify the `prep.py` and `exploration.py` to suit our inference requirements. While inferencing we don't have access to the target variable.\n",
    "2. We will also create new `infer.py` script to perform inference and save results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3b01f-f8a0-48d1-a6cc-1d5e51944497",
   "metadata": {},
   "outputs": [],
   "source": [
    "## validate inference data (Instructor guided)\n",
    "\n",
    "## validate cat levels (Instructor guided)\n",
    "\n",
    "## create X matrix (Instructor guided)\n",
    "\n",
    "## get summary stats (Instructor guided)\n",
    "\n",
    "## save prepared data and summary stats with appropriate time stamped directory (Instructor guided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7ce8b-20af-4f6e-8b2d-21579f30c916",
   "metadata": {},
   "source": [
    "Finally we can write an `infer.py` script. This script should\n",
    "\n",
    "1. Read the prepared inference data\n",
    "2. Read saved model\n",
    "3. Do inference\n",
    "4. Save inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa2cb6-ff5f-4b72-bea4-5bdc0ab1b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model (Instructor guided)\n",
    "\n",
    "## load data (Instructor guided)\n",
    "\n",
    "## get predictions (Instructor guided)\n",
    "\n",
    "## save predictions (Instructor guided)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
